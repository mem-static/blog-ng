{"componentChunkName":"component---src-templates-post-js","path":"/research/coding/pytorch/","result":{"data":{"site":{"siteMetadata":{"title":"mem 的小站"}},"post":{"id":"66a987a8-52e4-53fa-a2bc-19937261666d","excerpt":"1. 张量| Tensor  Useful Links  Tensor 创建方法：1.2 Tensor(张量)介绍 | PyTorch 学习笔记 : 被包装的 Tensor。 : data 的梯度。 : 创建 Tensor 所使用的 Function…","html":"<h2 id=\"anchor-14846835493cce25\" style=\"position: relative;\"><a href=\"#anchor-14846835493cce25\" aria-label=\"anchor 14846835493cce25 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 张量| Tensor</h2>\n<p><details  class=\"callout callout-type-info\"><summary > Useful Links</summary><div class=\"callout-content\"><p></p><ul>\n<li>Tensor 创建方法：<a href=\"https://pytorch.zhangxiann.com/1-ji-ben-gai-nian/1.2-tensor-zhang-liang-jie-shao#tensor-chuang-jian-de-fang-fa\">1.2 Tensor(张量)介绍 | PyTorch 学习笔记</a></li>\n</ul></div></details></p>\n<ul>\n<li><code class=\"language-text\">data</code>: 被包装的 Tensor。</li>\n<li><code class=\"language-text\">grad</code>: data 的梯度。</li>\n<li><code class=\"language-text\">grad_fn</code>: 创建 Tensor 所使用的 Function，是自动求导的关键，因为根据所记录的函数才能计算出导数。</li>\n<li><code class=\"language-text\">requires_grad</code>: 指示是否需要梯度，并不是所有的张量都需要计算梯度。</li>\n<li><code class=\"language-text\">is_leaf</code>: 指示是否叶子节点(张量)，叶子节点的概念在计算图中会用到，后面详细介绍。</li>\n<li><code class=\"language-text\">dtype</code>: 张量的数据类型，如 torch.FloatTensor，torch.cuda.FloatTensor。</li>\n<li><mark class=\"m-mark\"><code class=\"language-text\">shape</code>: 张量的形状。如 (64, 3, 224, 224)</mark></li>\n<li><code class=\"language-text\">device</code>: 张量所在设备 (CPU/GPU)，GPU 是加速计算的关键</li>\n</ul>\n<h3 id=\"anchor-cb23fb4cf73faf6f\" style=\"position: relative;\"><a href=\"#anchor-cb23fb4cf73faf6f\" aria-label=\"anchor cb23fb4cf73faf6f permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1.1. torch.tensor</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> pin_memory<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li><code class=\"language-text\">data</code>: 数据，可以是 list，numpy</li>\n<li><code class=\"language-text\">dtype</code>: 数据类型，默认与 data 的一致</li>\n<li><code class=\"language-text\">device</code>: 所在设备，cuda/cpu</li>\n<li><code class=\"language-text\">requires_grad</code>: 是否需要梯度</li>\n<li><code class=\"language-text\">pin_memory</code>: 是否存于锁页内存</li>\n</ul>\n<h2 id=\"anchor-04edb5283fccdaae\" style=\"position: relative;\"><a href=\"#anchor-04edb5283fccdaae\" aria-label=\"anchor 04edb5283fccdaae permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 张量操作 | Tensor Operations</h2>\n<p><details  class=\"callout callout-type-info\"><summary > Useful Links</summary><div class=\"callout-content\"><p></p><ul>\n<li><a href=\"https://pytorch.zhangxiann.com/1-ji-ben-gai-nian/1.3-zhang-liang-cao-zuo-yu-xian-xing-hui-gui#zhang-liang-de-cao-zuo\">1.3 张量操作与线性回归 | PyTorch 学习笔记</a></li>\n</ul></div></details></p>\n<h3 id=\"anchor-6280c64a36838aa7\" style=\"position: relative;\"><a href=\"#anchor-6280c64a36838aa7\" aria-label=\"anchor 6280c64a36838aa7 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.1. torch.cat</h3>\n<p>将张量按照 dim 维度进行拼接</p>\n<ul>\n<li><code class=\"language-text\">tensors</code>: 张量序列</li>\n<li><code class=\"language-text\">dim</code>: 要拼接的维度</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span>tensors<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> out<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><details  class=\"callout callout-type-example\"><summary ></summary><div class=\"callout-content\"><p></p><div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">t <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nt_0 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>t<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\nt_1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>t<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"t_0:{} shape:{}\\nt_1:{} shape:{}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>t_0<span class=\"token punctuation\">,</span> t_0<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> t_1<span class=\"token punctuation\">,</span> t_1<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div><div class=\"gatsby-highlight\" data-language=\"plain\"><pre class=\"language-plain\"><code class=\"language-plain\">t_0:tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]]) shape:torch.Size([4, 3])\nt_1:tensor([[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]]) shape:torch.Size([2, 6])</code></pre></div></div></details></p>\n<h3 id=\"anchor-503aa38cc2746af1\" style=\"position: relative;\"><a href=\"#anchor-503aa38cc2746af1\" aria-label=\"anchor 503aa38cc2746af1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.2. torch.stack</h3>\n<p>将张量在新创建的 dim 维度上进行拼接。（和 cat 的区别就在于这里需要创建一个新的维度——cat 操作对 dim 的影响是 sum，这里就相当于是 len）</p>\n<ul>\n<li><code class=\"language-text\">tensors</code>: 张量序列</li>\n<li><code class=\"language-text\">dim</code>: 要拼接的维度</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span>tensors<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> out<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><details  class=\"callout callout-type-example\"><summary ></summary><div class=\"callout-content\"><p></p><p>第一次指定拼接的维度 dim =2，结果的维度是 [2, 3, 3]。后面指定拼接的维度 dim =0，由于原来的 tensor 已经有了维度 0，因此会把 tensor 往后移动一个维度变为 [1,2,3]，再拼接变为 [3,2,3]。</p><div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">t <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># dim =2</span>\nt_stack <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>t<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nt_stack.shape:{}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>t_stack<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># dim =0</span>\nt_stack <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>t<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nt_stack.shape:{}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>t_stack<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div><div class=\"gatsby-highlight\" data-language=\"plain\"><pre class=\"language-plain\"><code class=\"language-plain\">t_stack.shape:torch.Size([2, 3, 3])\nt_stack.shape:torch.Size([3, 2, 3])</code></pre></div></div></details></p>\n<h2 id=\"anchor-a6aba5cd792b4bc1\" style=\"position: relative;\"><a href=\"#anchor-a6aba5cd792b4bc1\" aria-label=\"anchor a6aba5cd792b4bc1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 常用计算</h2>\n<table>\n<thead>\n<tr>\n<th>函数</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">torch.mm(input, mat2, *, out=None)</code><br>矩阵乘法 matrix multiplication<br></td>\n<td><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"bold\">A</mi><mtext>out</mtext></msub><mo>=</mo><msub><mi mathvariant=\"bold\">A</mi><mtext>input</mtext></msub><mo>×</mo><msub><mi mathvariant=\"bold\">A</mi><mtext>mat2</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}_{\\text{out}} = \\mathbf{A}_{\\text{input}} \\times \\mathbf{A}_{\\text{mat2}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8361em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">out</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9722em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3175em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">input</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8361em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">mat2</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"anchor-13751e1de006dd43\" style=\"position: relative;\"><a href=\"#anchor-13751e1de006dd43\" aria-label=\"anchor 13751e1de006dd43 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 计算图 | Compute Graph</h2>\n<p><details  class=\"callout callout-type-info\"><summary > Useful Links</summary><div class=\"callout-content\"><p></p><ul>\n<li>计算图机制讲解：<a href=\"https://zhuanlan.zhihu.com/p/33378444\">pytorch 的计算图 - 知乎</a></li>\n<li>一个生动的例子：<a href=\"https://zhuanlan.zhihu.com/p/598760275\">PyTorch | 简介计算图与动态图机制 - 知乎</a></li>\n<li><a href=\"https://pytorch.zhangxiann.com/1-ji-ben-gai-nian/1.4-ji-suan-tu-yu-dong-tai-tu-ji-zhi\">1.4 计算图与动态图机制 | PyTorch 学习笔记</a></li>\n</ul></div></details></p>\n<p>个人理解：可以把运算抽象成以下一个计算图，则根据一对边进行的运算就可以得到其偏导数是什么（比如 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>a</mi><mo>×</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y=a \\times b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span>，则 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mstyle displaystyle=\"true\" scriptlevel=\"0\"><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>a</mi></mrow></mfrac></mstyle><mo>=</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">\\dfrac{\\partial y}{\\partial  a}= b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.0574em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\">a</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span>，<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mstyle displaystyle=\"true\" scriptlevel=\"0\"><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>b</mi></mrow></mfrac></mstyle><mo>=</mo><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">\\dfrac{\\partial  y}{\\partial b}=a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.0574em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\">b</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span></span>）。为了求得导数，先进行一遍正向传播，求出中间变量的值是多少，再进行反向传播——沿着边的反方向走，经过边就乘上偏导数，几条反边指向同一个点则将他们求和。</p>\n<p><img src=\"https://img.memset0.cn/2025/01/21/1LMI18sY.png\"alt=\"\" style=\"width: 390px; \" ></p>\n<ul>\n<li>注意区别于 Tensorflow 的静态图机制，Pytorch 的计算图采用动态图机制。所以我们可以自然地在 Pytorch 中使用 <code class=\"language-text\">while</code> 而不用使用 <code class=\"language-text\">tf.while_loop</code>。</li>\n<li>为了节约内存，计算图在运算完成一次后会被释放，如果想要保留计算图以再次使用，需要使用 <code class=\"language-text\">loss.backward(retain_graph=True)</code> 代替 <code class=\"language-text\">loss.backward()</code>。</li>\n<li>默认情况下，Pytorch 只会保留叶子结点的梯度（<code class=\"language-text\">is_leaf</code> 为真的 tensor），如果需要保留中间节点的梯度，可使用 <code class=\"language-text\">a.retain_grad()</code> 在进行反向传播之前。</li>\n</ul>","tableOfContents":"<ul>\n<li>\n<p><a href=\"#anchor-14846835493cce25\">1. 张量| Tensor</a></p>\n<ul>\n<li><a href=\"#anchor-cb23fb4cf73faf6f\">1.1. torch.tensor</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#anchor-04edb5283fccdaae\">2. 张量操作 | Tensor Operations</a></p>\n<ul>\n<li><a href=\"#anchor-6280c64a36838aa7\">2.1. torch.cat</a></li>\n<li><a href=\"#anchor-503aa38cc2746af1\">2.2. torch.stack</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#anchor-a6aba5cd792b4bc1\">3. 常用计算</a></p>\n</li>\n<li>\n<p><a href=\"#anchor-13751e1de006dd43\">4. 计算图 | Compute Graph</a></p>\n</li>\n</ul>","frontmatter":{"title":"Pytorch 学习笔记","description":null},"fields":{"cover":null,"slug":"/research/coding/pytorch/","cssclasses":null,"isDoc":true,"authors":[],"createTime":null,"updateTime":null,"category":"[{\"name\":\"科研随记\",\"to\":\"/research/\"}]","propsJson":null}},"previous":null,"next":null},"pageContext":{"id":"66a987a8-52e4-53fa-a2bc-19937261666d","previousPostId":null,"nextPostId":null,"navJson":null}},"staticQueryHashes":[],"slicesMap":{}}