{"componentChunkName":"component---src-templates-post-js","path":"/research/paper-reading/deepwalk/","result":{"data":{"site":{"siteMetadata":{"title":"mem 的小站"}},"post":{"id":"85249dcd-0b94-56a5-8081-d0ffa2e4f747","excerpt":"论文：[[Bryan Perozzi, et al., 2014. DeepWalk - Online Learning of Social Representations]] 1. Features 2. Insights 2.1. Power Laws & Scale-free Graphs 幂律分布(Power…","html":"<ul>\n<li>论文：[[Bryan Perozzi, et al., 2014. DeepWalk - Online Learning of Social Representations]]</li>\n</ul>\n<h2 id=\"anchor-0f7c0c8c52c33276\" style=\"position: relative;\"><a href=\"#anchor-0f7c0c8c52c33276\" aria-label=\"anchor 0f7c0c8c52c33276 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Features</h2>\n<h2 id=\"anchor-75bae1c0c650d478\" style=\"position: relative;\"><a href=\"#anchor-75bae1c0c650d478\" aria-label=\"anchor 75bae1c0c650d478 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Insights</h2>\n<h3 id=\"anchor-c6d6455928936cbe\" style=\"position: relative;\"><a href=\"#anchor-c6d6455928936cbe\" aria-label=\"anchor c6d6455928936cbe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.1. Power Laws &#x26; Scale-free Graphs</h3>\n<p><strong>幂律分布(Power Law)</strong> 指 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo><mo>∝</mo><msup><mi>k</mi><mrow><mo>−</mo><mi>γ</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">P(k)\\propto k^{-\\gamma}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7713em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7713em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05556em;\">γ</span></span></span></span></span></span></span></span></span></span></span></span></span> 的概率分布，也叫长尾分布。在 NLP 中常用于指出大部分单词出现概率极低，只有少量单词出现概率较高。而在图中，用 <strong>无标度(scale-free)</strong> 图的特点则是网络中存在少数高度连接的枢纽节点和大量低度节点，典型例子包括社交网络、互联网、蛋白质相互作用网络等。即大部分节点度数极低，但存在少量度数极高的节点，也服从幂律分布。</p>\n<p><img src=\"https://img.memset0.cn/2025/01/25/KGchde3u.png\"alt=\"\" style=\"width: 675px; \" ></p>\n<h3 id=\"anchor-3194788a64b066d3\" style=\"position: relative;\"><a href=\"#anchor-3194788a64b066d3\" aria-label=\"anchor 3194788a64b066d3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.2. Skip-Gram</h3>\n<p>在 NLP 中有两个关于 <strong>词袋模型(bag of words)</strong> 的常用算法：</p>\n<ul>\n<li>CBOW：给定上下文词的情况下预测中心词（在平滑分布上表现更好）；</li>\n<li>Skip-Gram：给定中心词的情况下预测上下文词（在幂律分布上表现更好）。</li>\n</ul>\n<p>因为我们研究的无标度图及在无标度图上采样的短随机游走路径也服从幂律分布，故应采用 Skip-Gram 算法，其目标函数为：</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><munder><mrow><mi mathvariant=\"normal\">minimize</mi><mo>⁡</mo></mrow><mi mathvariant=\"normal\">Φ</mi></munder><mtext> </mtext><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>Pr</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>−</mo><mi>w</mi></mrow></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mi>w</mi></mrow></msub><mo stretchy=\"false\">}</mo><mo>∣</mo><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{minimize}_{\\Phi}\\space-\\log\\Pr\\left(\\{v_{i-w},\\cdots,v_{i-1},v_{i+1},\\cdots,v_{i+w}\\}\\mid\\Phi(v_i)\\right)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.4943em;vertical-align:-0.7443em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6679em;\"><span style=\"top:-2.3557em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">Φ</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\"><span class=\"mord mathrm\">minimize</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7443em;\"><span></span></span></span></span></span><span class=\"mspace\"> </span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">Pr</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">(</span><span class=\"mopen\">{</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mclose\">}</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord\">Φ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose delimcenter\" style=\"top:0em;\">)</span></span></span></span></span></span></div>\n<p>出于计算复杂度和实际可行性的考虑，我们假设上下文节点之间是相互独立的，从而可将目标函数简化为：</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><munder><mrow><mi mathvariant=\"normal\">minimize</mi><mo>⁡</mo></mrow><mi mathvariant=\"normal\">Φ</mi></munder><mtext> </mtext><mo>−</mo><munder><mo>∑</mo><mrow><msub><mi>u</mi><mi>k</mi></msub><mo>∈</mo><mtext>window</mtext><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></munder><mi>log</mi><mo>⁡</mo><mtext>Pr</mtext><mrow><mo fence=\"true\">(</mo><msub><mi>u</mi><mi>k</mi></msub><mo>∣</mo><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{\\text{minimize}}_{\\Phi}\\space - \\sum_{u_{k} \\in \\text{window}(v_{i})} \\log \\text{Pr} \\left( u_{k}\\mid \\Phi(v_{i}) \\right)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.566em;vertical-align:-1.516em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6679em;\"><span style=\"top:-2.3557em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">Φ</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\"><span class=\"mord text\"><span class=\"mord\">minimize</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7443em;\"><span></span></span></span></span></span><span class=\"mspace\"> </span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.05em;\"><span style=\"top:-1.809em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1512em;\"><span></span></span></span></span></span></span><span class=\"mrel mtight\">∈</span><span class=\"mord text mtight\"><span class=\"mord mtight\">window</span></span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3281em;\"><span style=\"top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mclose mtight\">)</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.516em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord text\"><span class=\"mord\">Pr</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord\">Φ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose delimcenter\" style=\"top:0em;\">)</span></span></span></span></span></span></div>\n<p>优化后的目标函数也方便使用层次化 softmax 算法进行优化。</p>\n<p>另外，Skip-Gram 算法也可采用 <strong>负采样(negative sampling)</strong> 技术优化，虽然在论文中没有提及。</p>\n<h3 id=\"anchor-351887352b66ca08\" style=\"position: relative;\"><a href=\"#anchor-351887352b66ca08\" aria-label=\"anchor 351887352b66ca08 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.3. Hierarchical Softmax</h3>\n<p>Skip-Gram 中要求我们得到“根据上下文填入单词”的似然概率，也就是“在随机游走路径填入节点”的似然概率，这里需要一个 <strong>归一化因子(partition function)</strong> 以从所有节点中选择（即词典就是点集，<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"script\">V</mi><mo>=</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V} = V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.08222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span>），而节点的总个数是很多的，这在节点数量很大时，会带来很大的计算代价。论文使用了 <strong>层次化 Softmax(Hierarchical Softmax)</strong> 的方式，根据节点在随机游走路径中出现的频率构建霍夫曼树，并通过树形结构计算 Softmax 函数。将 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi>V</mi><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|V|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">∣</span></span></span></span></span> 分类问题转化为若干个二分类问题，从而将单次计算的复杂度从 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∣</mi><mi>V</mi><mi mathvariant=\"normal\">∣</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(|V|)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">∣</span><span class=\"mclose\">)</span></span></span></span></span> 降低到 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>log</mi><mo>⁡</mo><mi mathvariant=\"normal\">∣</mi><mi>V</mi><mi mathvariant=\"normal\">∣</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(\\log |V|)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">∣</span><span class=\"mclose\">)</span></span></span></span></span>。</p>\n<p>若霍夫曼树上节点 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>u</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">u_{k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> 到根节点的路径为 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mn>0</mn></msub><mo separator=\"true\">,</mo><msub><mi>b</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><mo>⋯</mo><mtext> </mtext><mo separator=\"true\">,</mo><msub><mi>b</mi><mi>L</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(b_{0},b_{1},\\cdots,b_{L})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">0</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">⋯</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>（其中 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>b</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">b_{0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">0</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> 为根节点，<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>b</mi><mi>L</mi></msub><mo>=</mo><msub><mi>u</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">b_{L} = u_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>），则其概率为路径上每个二分类概率（往左子树走还是往右子树走）的乘积：</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>Pr</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>k</mi></msub><mo>∣</mo><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>Pr</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><msub><mi>b</mi><mi>l</mi></msub><mo>∣</mo><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><msub><mi>b</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Pr(u_k\\mid\\Phi(v_j))=\\prod_{l=1}^L\\Pr \\left( b_l\\mid\\Phi(v_j),b_{l-1} \\right)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">Pr</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\">Φ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mclose\">))</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.1304em;vertical-align:-1.3021em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.8479em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∏</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3021em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">Pr</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∣</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord\">Φ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\">)</span></span></span></span></span></span></div>\n<p><details  class=\"callout callout-type-example\"><summary > 普通 Softmax 代码实现</summary><div class=\"callout-content\"><p></p><ul>\n<li><strong>权重矩阵</strong>：\n<ul>\n<li><code class=\"language-text\">input_embeds</code>: 中心节点的嵌入矩阵，形状  <code class=\"language-text\">[num_nodes, embedding_dim]</code></li>\n<li><code class=\"language-text\">output_weights</code>: 上下文节点的权重矩阵，形状  <code class=\"language-text\">[num_nodes, embedding_dim]</code></li>\n</ul>\n</li>\n<li><strong>问题</strong>：计算  <code class=\"language-text\">output_weights</code>  与所有节点的点积时复杂度为  <code class=\"language-text\">O(num_nodes)</code>，无法扩展。</li>\n</ul><div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">DeepWalkSoftmax</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> num_nodes<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>num_nodes <span class=\"token operator\">=</span> num_nodes\n        self<span class=\"token punctuation\">.</span>embedding_dim <span class=\"token operator\">=</span> embedding_dim\n\n        <span class=\"token comment\"># 输入嵌入矩阵（中心节点）</span>\n        self<span class=\"token punctuation\">.</span>input_embeds <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>num_nodes<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 输出权重矩阵（上下文节点）</span>\n        self<span class=\"token punctuation\">.</span>output_weights <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>num_nodes<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> center_node<span class=\"token punctuation\">,</span> context_node<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 获取中心节点嵌入 [batch_size, embedding_dim]</span>\n        center_embed <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>input_embeds<span class=\"token punctuation\">(</span>center_node<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 计算所有节点的得分 [batch_size, num_nodes]</span>\n        scores <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>center_embed<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>output_weights<span class=\"token punctuation\">.</span>weight<span class=\"token punctuation\">.</span>T<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Softmax归一化</span>\n        probs <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>scores<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 提取目标上下文节点的概率</span>\n        target_probs <span class=\"token operator\">=</span> probs<span class=\"token punctuation\">[</span>torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span>probs<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> context_node<span class=\"token punctuation\">]</span>\n\n        <span class=\"token comment\"># 负对数似然损失</span>\n        loss <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>torch<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>target_probs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> loss\n\n<span class=\"token comment\"># 示例训练代码（小规模）</span>\nnum_nodes <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>   <span class=\"token comment\"># 假设图中有1000个节点</span>\nembedding_dim <span class=\"token operator\">=</span> <span class=\"token number\">128</span>\n\nmodel <span class=\"token operator\">=</span> DeepWalkSoftmax<span class=\"token punctuation\">(</span>num_nodes<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 模拟数据：中心节点和上下文节点对</span>\ncenter_nodes <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> num_nodes<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># batch_size=32</span>\ncontext_nodes <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> num_nodes<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 训练步骤</span>\nmodel<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nloss <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>center_nodes<span class=\"token punctuation\">,</span> context_nodes<span class=\"token punctuation\">)</span>\nloss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Loss: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span></code></pre></div></div></details></p>\n<p><details  class=\"callout callout-type-example\"><summary > 层次化 Softmax（使用霍夫曼树）代码实现</summary><div class=\"callout-content\"><p></p><ul>\n<li><strong>权重矩阵</strong>：\n<ul>\n<li><code class=\"language-text\">input_embeds</code>: 同传统 Softmax</li>\n<li><code class=\"language-text\">inner_params</code>: 霍夫曼树内部节点的参数列表，长度  <code class=\"language-text\">num_nodes-1</code></li>\n</ul>\n</li>\n<li><strong>优化</strong>：每个目标节点的损失计算仅需遍历其路径（长度  <code class=\"language-text\">O(log num_nodes)</code>），复杂度显著降低。</li>\n</ul><div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">HuffmanNode</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"霍夫曼树节点辅助类\"\"\"</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> <span class=\"token builtin\">id</span><span class=\"token punctuation\">,</span> is_leaf<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span><span class=\"token builtin\">id</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">id</span>\n        self<span class=\"token punctuation\">.</span>is_leaf <span class=\"token operator\">=</span> is_leaf\n        self<span class=\"token punctuation\">.</span>left <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span>\n        self<span class=\"token punctuation\">.</span>right <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span>\n        self<span class=\"token punctuation\">.</span>parent <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span>\n        self<span class=\"token punctuation\">.</span>path <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>  <span class=\"token comment\"># 路径方向序列（'left'或'right'）</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">DeepWalkHierarchicalSoftmax</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> num_nodes<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">,</span> huffman_tree<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>num_nodes <span class=\"token operator\">=</span> num_nodes\n        self<span class=\"token punctuation\">.</span>embedding_dim <span class=\"token operator\">=</span> embedding_dim\n        self<span class=\"token punctuation\">.</span>tree <span class=\"token operator\">=</span> huffman_tree  <span class=\"token comment\"># 预构建的霍夫曼树</span>\n\n        <span class=\"token comment\"># 中心节点嵌入矩阵</span>\n        self<span class=\"token punctuation\">.</span>input_embeds <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>num_nodes<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 内部节点参数（每个内部节点对应一个向量）</span>\n        self<span class=\"token punctuation\">.</span>inner_params <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ParameterList<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n            nn<span class=\"token punctuation\">.</span>Parameter<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>embedding_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_nodes <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 二叉树内部节点数为n-1</span>\n        <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> center_node<span class=\"token punctuation\">,</span> target_node<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># 获取中心节点嵌入 [batch_size, embedding_dim]</span>\n        center_embed <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>input_embeds<span class=\"token punctuation\">(</span>center_node<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 获取目标节点的路径（假设已预计算）</span>\n        path <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>tree<span class=\"token punctuation\">.</span>get_path<span class=\"token punctuation\">(</span>target_node<span class=\"token punctuation\">)</span>\n\n        total_loss <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span>\n        <span class=\"token keyword\">for</span> node<span class=\"token punctuation\">,</span> direction <span class=\"token keyword\">in</span> path<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> node<span class=\"token punctuation\">.</span>is_leaf<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">continue</span>\n\n            <span class=\"token comment\"># 获取内部节点参数 [embedding_dim]</span>\n            theta <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>inner_params<span class=\"token punctuation\">[</span>node<span class=\"token punctuation\">.</span><span class=\"token builtin\">id</span><span class=\"token punctuation\">]</span>\n\n            <span class=\"token comment\"># 计算概率</span>\n            logit <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>center_embed<span class=\"token punctuation\">,</span> theta<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># [batch_size]</span>\n            prob <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sigmoid<span class=\"token punctuation\">(</span>logit<span class=\"token punctuation\">)</span>\n\n            <span class=\"token comment\"># 根据方向计算损失</span>\n            <span class=\"token keyword\">if</span> direction <span class=\"token operator\">==</span> <span class=\"token string\">'left'</span><span class=\"token punctuation\">:</span>\n                loss <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>torch<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>prob <span class=\"token operator\">+</span> <span class=\"token number\">1e-10</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                loss <span class=\"token operator\">=</span> <span class=\"token operator\">-</span>torch<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> prob <span class=\"token operator\">+</span> <span class=\"token number\">1e-10</span><span class=\"token punctuation\">)</span>\n\n            total_loss <span class=\"token operator\">+=</span> loss<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> total_loss\n\n<span class=\"token comment\"># 示例霍夫曼树构建（简化版）</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">build_demo_huffman_tree</span><span class=\"token punctuation\">(</span>num_nodes<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># 假设所有节点路径已预计算（实际需按频率构建）</span>\n    root <span class=\"token operator\">=</span> HuffmanNode<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n    leaf_nodes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>HuffmanNode<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span>i<span class=\"token punctuation\">,</span> is_leaf<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_nodes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token comment\"># 示例：为第一个叶子节点分配路径 root -> left -> left</span>\n    leaf <span class=\"token operator\">=</span> leaf_nodes<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    leaf<span class=\"token punctuation\">.</span>path <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">(</span>root<span class=\"token punctuation\">,</span> <span class=\"token string\">'left'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">(</span>root<span class=\"token punctuation\">.</span>left<span class=\"token punctuation\">,</span> <span class=\"token string\">'left'</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> root\n\n<span class=\"token comment\"># 训练代码</span>\nnum_nodes <span class=\"token operator\">=</span> <span class=\"token number\">1000</span>\nembedding_dim <span class=\"token operator\">=</span> <span class=\"token number\">128</span>\nhuffman_tree <span class=\"token operator\">=</span> build_demo_huffman_tree<span class=\"token punctuation\">(</span>num_nodes<span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> DeepWalkHierarchicalSoftmax<span class=\"token punctuation\">(</span>num_nodes<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">,</span> huffman_tree<span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 模拟数据</span>\ncenter_nodes <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> num_nodes<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># batch_size=32</span>\ntarget_nodes <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> num_nodes<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 目标节点</span>\n\n<span class=\"token comment\"># 训练步骤</span>\nmodel<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nloss <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>center_nodes<span class=\"token punctuation\">,</span> target_nodes<span class=\"token punctuation\">)</span>\nloss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Hierarchical Loss: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span></code></pre></div></div></details></p>\n<h2 id=\"anchor-4185978965105a1c\" style=\"position: relative;\"><a href=\"#anchor-4185978965105a1c\" aria-label=\"anchor 4185978965105a1c permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. References</h2>\n<ul>\n<li>原论文。</li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/37477611\">word2vec 中 cbow 与 skip-gram 的比较 - 知乎</a></li>\n</ul>","tableOfContents":"<ul>\n<li>\n<p><a href=\"#anchor-0f7c0c8c52c33276\">1. Features</a></p>\n</li>\n<li>\n<p><a href=\"#anchor-75bae1c0c650d478\">2. Insights</a></p>\n<ul>\n<li><a href=\"#anchor-c6d6455928936cbe\">2.1. Power Laws &#x26; Scale-free Graphs</a></li>\n<li><a href=\"#anchor-3194788a64b066d3\">2.2. Skip-Gram</a></li>\n<li><a href=\"#anchor-351887352b66ca08\">2.3. Hierarchical Softmax</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#anchor-4185978965105a1c\">3. References</a></p>\n</li>\n</ul>","frontmatter":{"title":"DeepWalk 精读","description":null},"fields":{"cover":null,"slug":"/research/paper-reading/deepwalk/","cssclasses":null,"isDoc":true,"authors":[],"createTime":"2025 年 1 月 25 日","updateTime":"2025 年 1 月 25 日","category":"[{\"name\":\"科研随记\",\"to\":\"/research/\"}]","propsJson":null}},"previous":null,"next":null},"pageContext":{"id":"85249dcd-0b94-56a5-8081-d0ffa2e4f747","previousPostId":null,"nextPostId":null,"navJson":"[{\"slug\":\"/research\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/index.md\",\"title\":\"导航\"},{\"title\":\"神经网络与深度学习\",\"children\":[{\"slug\":\"/research/nndl/introduction\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/introduction.md\",\"title\":\"机器学习概述\"},{\"slug\":\"/research/nndl/linear-model\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/linear-model.md\",\"title\":\"线性模型\"},{\"slug\":\"/research/nndl/fnn\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/fnn.md\",\"title\":\"前馈神经网络\"},{\"slug\":\"/research/nndl/cnn\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/cnn.md\",\"title\":\"卷积神经网络\"},{\"slug\":\"/research/nndl/rnn\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/rnn.md\",\"title\":\"循环神经网络\"},{\"slug\":\"/research/nndl/attention\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/attention.md\",\"title\":\"注意力机制\"},{\"slug\":\"/research/nndl/rl\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/rl.md\",\"title\":\"强化学习\"},{\"slug\":\"/research/nndl/generative-model\",\"file\":\"/home/runner/work/gatsby-blog/gatsby-blog/content/research/nndl/generative-model.md\",\"title\":\"概率生成模型\"}]}]"}},"staticQueryHashes":[],"slicesMap":{}}